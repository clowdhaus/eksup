{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"EKS Cluster Upgrade Guidance","text":""},{"location":"#why-is-this-needed","title":"Why Is This Needed","text":"<p>Kubernetes releases a new version approximately every 4 months. Each minor version is supported for 12 months after it's first released by the Kubernetes community, and Amazon EKS supports a Kubernetes version for 14 months once made available. In line with the Kubernetes community support for versions, Amazon EKS is committed to supporting at least four versions of Kubernetes at any given time. This means that Amazon EKS users need to be prepared to upgrade their cluster version(s) at least once a year. However, there are a number of factors that can make each upgrade different and unique that users will need to evaluate prior to each upgrade. Factors that can change between each upgrade cycle include:</p> <ul> <li>New team members who are inexperienced with the upgrade process, and/or prior team members who have experience in cluster upgrades are no longer on the team</li> <li>Different Kubernetes APIs are marked as deprecated or removed from the next release</li> <li>Kubernetes resources that were previously provided by Kubernetes \"in-tree\" are now provided as external resources (i.e - moving Kubernetes in-tree cloud provider code out to their respective standalone projects such as ALB ingress controller \"in-tree\" to the external ALB load balancer controller)</li> <li>Various changes and deprecations in the components used by Kubernetes (i.e - moving from <code>kube-dns</code> to <code>CoreDNS</code>, moving from Docker engine to <code>containerd</code> for container runtime, dropping support for <code>dockershim</code>, etc.)</li> <li>Changes in your applications, your architecture, or the amount of traffic your clusters are handling. Over time, the number of available IPs for the cluster resources may shrink, stateful workloads may have been added to the cluster, etc., and these factors can influence the upgrade process.</li> </ul>"},{"location":"#what-is-eksup","title":"What Is <code>eksup</code>","text":"<p><code>eksup</code> is a CLI that helps users prepare for a cluster upgrade - providing users as much relevant information as possible for their upgrade.</p> <p><code>eksup</code> gives users the ability to analyze their cluster(s) against the next version of Kubernetes, highlighting any findings that may affect the upgrade process. In addition, <code>eksup</code> has the ability to generate a playbook tailored to the cluster analyzed that details the process for upgrading the cluster, including the findings that require remediation. The playbook output allows users to edit the upgrade steps to suit their cluster configuration and business requirements plus capture any specific learnings during the upgrade process. Since most users typically perform upgrades on nonproduction clusters first, any additional steps or call-outs that are discovered during the upgrade process can be captured and used to improve the upgrade process for their production clusters. Users are encouraged to save their playbooks as historical artifacts for future reference to ensure that with each cycle, the team has a better understanding of the upgrade process and more confidence in swiftly working through cluster upgrades before their Kubernetes version support expires.</p>"},{"location":"#what-it-is-not","title":"What It Is NOT","text":"<ul> <li><code>eksup</code> is not a tool that will perform the cluster upgrade. It is assumed that clusters are generally created using an infrastructure as code approach through tools such as Terraform, <code>eksctl</code>, or CloudFormation. Therefore, users are encouraged to use those tools to perform the upgrade to avoid any resource definition conflicts.</li> <li>It does not perform any modifications on the resources it identifies as needing, or recommending, changes. Again, following the approach of infrastructure as code, users are encouraged to make these changes through their normal change control process at the appropriate time in the upgrade process.</li> <li>In the future, <code>eksup</code> may provide functionality to help in converting a Kubernetes manifest definition from one API version to the next. However, this will occur on the users local filesystem and not against a live cluster. <code>eksup</code> will always operate from the perspective of infrastructure as code; any feature requests that support this tenant are encouraged.</li> </ul>"},{"location":"#symbol-table","title":"Symbol Table","text":"Symbol Description \u2139\ufe0f Informational - users are encouraged to familiarize themselves with the information but no action is required to upgrade \u26a0\ufe0f Recommended - users are encouraged to evaluate the recommendation and determine if it is applicable and whether or not to act upon that recommendation. Not remediating the finding does not prevent the upgrade from occurring. \u274c Required - users must remediate the finding prior to upgrading to be able to perform the upgrade and avoid downtime or disruption"},{"location":"info/checks/","title":"Checks","text":"<p>The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in BCP 14 [RFC 2119] [RFC 8174] when, and only when, they appear in all capitals, as shown here.</p> <p>If a check fails, it is reported as a finding. Each check will have a remediation type - either recommended or required. A recommended remediation is one that is recommended to be performed, but is not required to be performed.</p> <ul> <li>\u26a0\ufe0f Recommended: A finding that users are encouraged to evaluate the recommendation and determine if it is applicable and whether or not to act upon that recommendation. Not remediating the finding does not prevent the upgrade from occurring.</li> <li>\u274c Required: A finding that requires remediation prior to upgrading to be able to perform the upgrade and avoid downtime or disruption</li> </ul> <p>See the symbol table for further details on the symbols used throughout the documentation.</p>"},{"location":"info/checks/#summary","title":"Summary","text":"Code Description Status Applicable Versions <code>AWS001</code> Insufficient available subnet IPs for nodes Active All versions <code>AWS002</code> Insufficient available subnet IPs for pods (custom networking) Active All versions <code>AWS003</code> Insufficient EC2 service limits Active All versions <code>AWS004</code> Insufficient EBS GP2 service limits Active All versions <code>AWS005</code> Insufficient EBS GP3 service limits Active All versions <code>EKS001</code> Insufficient available subnet IPs for control plane ENIs Active All versions <code>EKS002</code> Health issue(s) reported by the EKS control plane Active All versions <code>EKS003</code> Health issue(s) reported by the EKS managed node group Active All versions <code>EKS004</code> Health issue(s) reported by the EKS addon Active All versions <code>EKS005</code> EKS addon incompatible with targeted Kubernetes version Active All versions <code>EKS006</code> EKS managed node group has pending launch template update(s) Active All versions <code>EKS007</code> Self-managed node group has pending launch template update(s) Active All versions <code>EKS008</code> AL2 AMI deprecation (deprecated in 1.32, removed in 1.33+) Active 1.32+ <code>EKS009</code> EKS upgrade readiness insight Active All versions <code>EKS010</code> EKS cluster misconfiguration insight Active All versions <code>K8S001</code> Kubernetes version skew between control plane and node Active All versions <code>K8S002</code> Insufficient number of .spec.replicas (configurable) Active All versions <code>K8S003</code> Insufficient .spec.minReadySeconds Active All versions <code>K8S004</code> Missing PodDisruptionBudget Active All versions <code>K8S005</code> Pod distribution settings put availability at risk Active All versions <code>K8S006</code> Missing readinessProbe on containers Active All versions <code>K8S007</code> TerminationGracePeriodSeconds is set to zero Active All versions <code>K8S008</code> Mounts docker.sock or dockershim.sock Active All versions <code>K8S009</code> Pod security policies present (removed in 1.25) Retired Up to 1.24 <code>K8S010</code> EBS CSI driver not installed Retired Up to 1.24 <code>K8S011</code> kube-proxy version skew with kubelet Active All versions <code>K8S012</code> kube-proxy IPVS mode deprecated (1.35+, removed 1.36) Active 1.35+ <code>K8S013</code> Ingress NGINX controller retirement (1.35+) Active 1.35+"},{"location":"info/checks/#amazon","title":"Amazon","text":"<p>Checks that are not specific to Amazon EKS or Kubernetes</p>"},{"location":"info/checks/#aws001","title":"AWS001","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>There MUST be a sufficient quantity of IPs available for the nodes to support the upgrade.</p> <p>If custom networking is enabled, the results represent the number of IPs available in the subnets used by the EC2 instances. Otherwise, the results represent the number of IPs available in the subnets used by both the EC2 instances and the pods.</p>"},{"location":"info/checks/#aws002","title":"AWS002","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>There MUST be a sufficient quantity of IPs available for the pods to support the upgrade.</p> <p>This check is used when custom networking is enabled since the IPs used by pods are coming from subnets different from those used by the EC2 instances themselves.</p>"},{"location":"info/checks/#aws003","title":"AWS003","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>There MUST be sufficient EC2 instance service limits to support the upgrade. During an upgrade, additional instances may be launched temporarily (e.g., by managed node groups or auto-scaling groups), and hitting service limits could prevent new nodes from joining the cluster.</p>"},{"location":"info/checks/#aws004","title":"AWS004","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>There MUST be sufficient EBS GP2 volume service limits to support the upgrade. Persistent volumes backed by GP2 may need to be re-attached to new nodes during the upgrade process.</p>"},{"location":"info/checks/#aws005","title":"AWS005","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>There MUST be sufficient EBS GP3 volume service limits to support the upgrade. Persistent volumes backed by GP3 may need to be re-attached to new nodes during the upgrade process.</p>"},{"location":"info/checks/#amazon-eks","title":"Amazon EKS","text":"<p>Checks that are specific to Amazon EKS</p>"},{"location":"info/checks/#eks001","title":"EKS001","text":"<p>\u274c Remediation required</p> <p>There MUST be at least 2 subnets in different availability zones, each with at least 5 available IPs for the control plane to upgrade.</p>"},{"location":"info/checks/#eks002","title":"EKS002","text":"<p>\u274c Remediation required</p> <p>Control plane MUST NOT have any reported health issues.</p>"},{"location":"info/checks/#eks003","title":"EKS003","text":"<p>\u274c Remediation required</p> <p>EKS managed nodegroup MUST NOT have any reported health issues.</p> <p>This does not include self-managed nodegroups or Fargate profiles; those are not currently supported by the AWS API to report health issues.</p>"},{"location":"info/checks/#eks004","title":"EKS004","text":"<p>\u274c Remediation required</p> <p>EKS addon MUST NOT have any reported health issues.</p>"},{"location":"info/checks/#eks005","title":"EKS005","text":"<p>\u274c Remediation required</p> <p>EKS addon version MUST be within the supported range.</p> <p>The addon MUST be updated to a version that is supported by the target Kubernetes version prior to upgrading.</p> <p>\u26a0\ufe0f Remediation recommended</p> <p>The target Kubernetes version default addon version is newer than the current addon version.</p> <p>For example, if the default addon version of CoreDNS for Kubernetes <code>v1.24</code> is <code>v1.8.7-eksbuild.3</code> and the current addon version is <code>v1.8.4-eksbuild.2</code>, while the current version is supported on Kubernetes <code>v1.24</code>, it is RECOMMENDED to update the addon to <code>v1.8.7-eksbuild.3</code> during the upgrade.</p>"},{"location":"info/checks/#eks006","title":"EKS006","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>EKS managed nodegroups SHOULD use the latest launch template version and there SHOULD NOT be any pending updates for the nodegroup.</p> <p>Users are encouraged to evaluate if remediation is warranted or not and whether to update to the latest launch template version prior to upgrading. If there are pending updates, this could potentially introduce additional changes to the nodegroup during the upgrade.</p>"},{"location":"info/checks/#eks007","title":"EKS007","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>Self-managed nodegroups SHOULD use the latest launch template version and there SHOULD NOT be any pending updates for the nodegroup.</p> <p>Users are encouraged to evaluate if remediation is warranted or not and whether to update to the latest launch template version prior to upgrading. If there are pending updates, this could potentially introduce additional changes to the nodegroup during the upgrade.</p>"},{"location":"info/checks/#eks008","title":"EKS008","text":"<p>EKS managed nodegroups MUST NOT use AL2 (Amazon Linux 2) AMI types. AL2 AMIs are deprecated starting in Kubernetes 1.32 and are no longer supported in 1.33+. Users MUST migrate to AL2023 or Bottlerocket AMI types.</p> <p>\u274c Remediation required</p> <p>For clusters upgrading to Kubernetes <code>v1.33</code> or later \u2014 AL2 AMI types MUST NOT be used as they are no longer supported.</p> <p>\u26a0\ufe0f Remediation recommended</p> <p>For clusters upgrading to Kubernetes <code>v1.32</code> \u2014 AL2 AMI types are deprecated and migration SHOULD be completed before they become unsupported.</p> <p>Amazon Linux 2 end of standard support</p>"},{"location":"info/checks/#eks009","title":"EKS009","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>EKS upgrade readiness insights are reported by the EKS API to help identify potential issues before upgrading. These insights are specific to upgrade compatibility and cover deprecated APIs, unsupported configurations, and other upgrade blockers detected by the EKS service.</p> <p>Users SHOULD review and address any upgrade readiness insights before proceeding with the cluster upgrade.</p> <p>Amazon EKS cluster insights</p>"},{"location":"info/checks/#eks010","title":"EKS010","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>EKS cluster misconfiguration insights are reported by the EKS API to identify configuration issues that may affect cluster health or functionality. Unlike upgrade readiness insights (EKS009), these are not specific to a particular upgrade but reflect general best practices and misconfigurations.</p> <p>Users SHOULD review and address any cluster misconfiguration insights to maintain cluster health.</p> <p>Amazon EKS cluster insights</p>"},{"location":"info/checks/#kubernetes","title":"Kubernetes","text":"<p>Checks that are specific to Kubernetes, regardless of the underlying platform provider.</p> <p>Table below shows the checks that are applicable, or not, to the respective Kubernetes resource.</p> Check Deployment ReplicaSet ReplicationController StatefulSet Job CronJob Daemonset <code>K8S001</code> \udb40\udc2d\u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 <code>K8S002</code> \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>K8S003</code> \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>K8S004</code> \u2705 \u2705 \u274c \u2705 \u274c \u274c \u274c <code>K8S005</code> \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>K8S006</code> \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>K8S007</code> \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>K8S008</code> \u274c \u274c \u274c \u2705 \u274c \u274c \u274c <code>K8S011</code> \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 <code>K8S012</code> \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 <code>K8S013</code> \u2705 \u274c \u274c \u274c \u274c \u274c \u2705"},{"location":"info/checks/#k8s001","title":"K8S001","text":"<p>\u274c Remediation required</p> <p>The version skew between the control plane (API Server) and the data plane (kubelet) MUST NOT violate the Kubernetes version skew policy, either currently or after the control plane has been upgraded.</p> <p>The data plane nodes MUST be upgraded to at least within 3 minor versions of the control plane version in order to stay within the version skew policy through the upgrade; it is RECOMMENDED to upgrade the data plane nodes to the same version as the control plane.</p> <p>\u26a0\ufe0f Remediation recommended</p> <p>There is a version skew between the control plane (API Server) and the data plane (kubelet).</p> <p>While Kubernetes does support a version skew of n-3 between the API Server and kubelet, the data plane nodes SHOULD be upgraded to the same version as the control plane.</p> <p>Kubernetes version skew policy</p>"},{"location":"info/checks/#k8s002","title":"K8S002","text":"<p>\u274c Remediation required</p> <p>There MUST be at least the configured minimum number of replicas specified for the resource. The default minimum is 2 replicas, which can be customized via <code>.eksup.yaml</code>.</p> <pre><code>---\nspec:\n  replicas: 2 # &gt;= configured minimum (default: 2)\n</code></pre> <p>Multiple replicas, along with the use of <code>PodDisruptionBudget</code>, are REQUIRED to ensure high availability during the upgrade.</p> <p>The minimum replica threshold, ignored resources, and per-resource overrides can be configured:</p> <pre><code>checks:\n  K8S002:\n    min_replicas: 3  # Global minimum replica threshold (default: 2)\n    ignore:          # Resources to skip entirely\n      - name: metrics-server\n        namespace: kube-system\n    overrides:       # Per-resource threshold overrides\n      - name: critical-app\n        namespace: production\n        min_replicas: 5\n</code></pre> <p>EKS Best Practices - Reliability</p>"},{"location":"info/checks/#k8s003","title":"K8S003","text":"<p>\u274c Remediation required</p> <p><code>minReadySeconds</code> MUST be set to a value greater than 0 seconds for <code>StatefulSet</code></p> <p>You can read more about why this is necessary for <code>StatefulSet</code> here</p> <p>\u26a0\ufe0f Remediation recommended</p> <p><code>minReadySeconds</code> SHOULD be set to a value greater than 0 seconds for <code>Deployment</code>, <code>ReplicaSet</code>, <code>ReplicationController</code></p>"},{"location":"info/checks/#k8s004","title":"K8S004","text":"<p>\u274c Remediation required</p> <p>At least one <code>podDisruptionBudget</code> MUST cover the workload, and at least one of <code>minAvailable</code> or <code>maxUnavailable</code> MUST be set.</p> <p>The Kubernetes eviction API is the preferred method for draining nodes for replacement during an upgrade. The eviction API respects <code>PodDisruptionBudget</code> and will not evict pods that would violate the <code>PodDisruptionBudget</code> to ensure application availability, when specified.</p> <p>Resources can be excluded from this check via <code>.eksup.yaml</code>:</p> <pre><code>checks:\n  K8S004:\n    ignore:          # Resources to skip PDB check\n      - name: singleton-worker\n        namespace: batch\n</code></pre>"},{"location":"info/checks/#k8s005","title":"K8S005","text":"<p>\u274c Remediation required</p> <p>Either <code>.spec.affinity.podAntiAffinity</code> or <code>.spec.topologySpreadConstraints</code> MUST be set to avoid multiple pods from the same workload from being scheduled on the same node.</p> <p><code>topologySpreadConstraints</code> are preferred over affinity, especially for larger clusters:</p> <ul> <li> <p>Inter-pod affinity and anti-affinity</p> <p>Note: Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes.</p> </li> </ul> <p>Types of inter-pod affinity and anti-affinity</p> <p>Pod Topology Spread Constraints</p>"},{"location":"info/checks/#k8s006","title":"K8S006","text":"<p>\u274c Remediation required</p> <p>A <code>readinessProbe</code> MUST be set to ensure traffic is not routed to pods before they are ready following their re-deployment from a node replacement.</p>"},{"location":"info/checks/#k8s007","title":"K8S007","text":"<p>\u274c Remediation required</p> <p>The <code>StatefulSet</code> MUST NOT specify a <code>TerminationGracePeriodSeconds</code> of 0.</p> <ul> <li> <p>Deployment and Scaling Guarantees</p> <p>The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to force deleting StatefulSet Pods.</p> </li> </ul> <p>Force Delete StatefulSet Pods</p>"},{"location":"info/checks/#k8s008","title":"K8S008","text":"<p>Pod volumes MUST NOT mount the <code>docker.sock</code> file with the removal of the Dockershim starting in Kubernetes <code>v1.24</code>.</p> <p>\u274c Remediation required</p> <p>For clusters on Kubernetes <code>v1.23</code> \u2014 Pod volumes MUST NOT mount the <code>docker.sock</code> file.</p> <p>\u26a0\ufe0f Remediation recommended</p> <p>For clusters on Kubernetes &lt;<code>v1.22</code> \u2014 Pod volumes SHOULD NOT mount the <code>docker.sock</code> file.</p> <p>Dockershim Removal FAQ</p> <p>Detector for Docker Socket (DDS)</p>"},{"location":"info/checks/#k8s011","title":"K8S011","text":"<p>\u274c Remediation required</p> <p><code>kube-proxy</code> on an Amazon EKS cluster MUST follow the same compatibility and skew policy as Kubernetes:</p> <ul> <li>It MUST NOT be newer than the minor version of your cluster's control plane.</li> <li>Its version MUST NOT be more than three minor versions older than your control plane (API server). For example, if your control plane is running Kubernetes <code>1.25</code>, then the kube-proxy minor version MUST NOT be older than <code>1.22</code>.</li> </ul> <p>If you recently updated your cluster to a new Kubernetes minor version, then update your Amazon EC2 nodes (i.e. - <code>kubelet</code>) to the same minor version before updating <code>kube-proxy</code> to the same minor version as your nodes. The order of operations during an upgrade are as follows:</p> <pre><code>1. Update the control plane to the new Kubernetes minor version\n2. Update the nodes, which updates `kubelet`, to the new Kubernetes minor version\n3. Update `kube-proxy` to the new Kubernetes minor version\n</code></pre>"},{"location":"info/checks/#k8s012","title":"K8S012","text":"<p><code>kube-proxy</code> IPVS proxy mode is deprecated starting in Kubernetes <code>v1.35</code> and will be removed in <code>v1.36</code>. Clusters using IPVS mode MUST migrate to <code>iptables</code> or <code>nftables</code> proxy mode.</p> <p>\u274c Remediation required</p> <p>For clusters upgrading to Kubernetes <code>v1.36</code> or later \u2014 IPVS proxy mode MUST NOT be used as it is removed.</p> <p>\u26a0\ufe0f Remediation recommended</p> <p>For clusters upgrading to Kubernetes <code>v1.35</code> \u2014 IPVS proxy mode is deprecated and SHOULD be migrated.</p> <p>Kubernetes kube-proxy documentation</p>"},{"location":"info/checks/#k8s013","title":"K8S013","text":"<p>\u26a0\ufe0f Remediation recommended</p> <p>The Kubernetes community Ingress NGINX controller (<code>registry.k8s.io/ingress-nginx/controller</code> or <code>k8s.gcr.io/ingress-nginx/controller</code>) has been retired. Users running this controller SHOULD migrate to an actively maintained ingress controller such as the AWS Load Balancer Controller or a third-party alternative.</p> <p>This check scans Deployments and DaemonSets for container images referencing the retired Ingress NGINX controller.</p> <p>Ingress NGINX Controller</p>"},{"location":"info/checks/#retired-checks","title":"Retired Checks","text":"<p>The following checks have been retired and are no longer evaluated. They remain documented here for reference.</p>"},{"location":"info/checks/#k8s009","title":"K8S009","text":"<p>Retired</p> <p>This check applied to Kubernetes versions up to 1.24 and is no longer relevant for supported cluster versions.</p> <p>Pod security policies were removed in Kubernetes <code>v1.25</code>. Clusters that previously relied on <code>PodSecurityPolicy</code> resources needed to migrate to the built-in Pod Security Admission controller.</p> <p>Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</p>"},{"location":"info/checks/#k8s010","title":"K8S010","text":"<p>Retired</p> <p>This check applied to Kubernetes versions up to 1.24 and is no longer relevant for supported cluster versions.</p> <p>The in-tree Amazon EBS storage provisioner was deprecated and clusters upgrading to version <code>v1.23</code> needed to install the Amazon EBS CSI driver first.</p> <p>Amazon EBS CSI migration frequently asked questions</p>"},{"location":"info/design/","title":"Design","text":""},{"location":"info/design/#goals","title":"Goals","text":"<p>There is one primary goal for <code>eksup</code>:</p> <p>Empower users to routinely upgrade their EKS cluster(s) while avoiding downtime and/or disruption.</p> <p>To aid in that goal, the following are supporting goals or tenants:</p> <ol> <li>Intended for use on Amazon EKS clusters; there are no guarantees that this CLI will or will not work on other Kubernetes clusters at this time. The CLI will focus on EKS to avoid the need to support multiple Kubernetes distributions and their associated cloud controller resources and instead focus on the aspects that are specific to EKS and how it manages the Kubernetes experience for users. The CLI should offer native support for:<ul> <li>Amazon EKS, Amazon EKS on Outposts (local and extended clusters), and [Not yet supported] EKS-A (EKS Anywhere)</li> <li>Amazon EKS managed node groups, self-managed node groups, and Amazon EKS Fargate profiles</li> <li>EKS addons</li> </ul> </li> <li>It is designed to produce the least amount of load on the API Server when discovering and analyzing cluster resources. However, this can have some tradeoffs in terms of accuracy and completeness of the information provided to the user</li> <li>It should provide as much relevant information as possible to the user regarding the state of the cluster prior to upgrading. This includes scoping the information provided to the user to only that which is relevant for upgrading from their current Kubernetes version to the intended target Kubernetes version.</li> <li>It should support the following use cases:<ul> <li>A one-off analysis to create a report of the cluster state prior to upgrading along with steps to take to upgrade the cluster (i.e. - analyze the cluster and generate an upgrade playbook).</li> <li>A one-off analysis to report on the state of the cluster for potential issues and/or recommendations prior to upgrading. This is generally a CLI invocation that prints the analysis to the console (stdout) and exits.</li> <li>Continuous analysis of the cluster state for potential issues and/or recommendations that runs from within the cluster that is being reported on. Results can be sent to stdout where they can be picked up from a log aggregator or sent to a remote location, such as Amazon S3, where they can be analyzed and acted upon. This process supports n-number of clusters across m-number of accounts to better aid in multi-cluster management as well as alerting to ensure enough advance notice is given for users to prepare and schedule the pending upgrade before end of support is reached.</li> </ul> </li> </ol>"},{"location":"info/design/#architecture","title":"Architecture","text":""},{"location":"info/design/#high-level-diagram","title":"High Level Diagram","text":""},{"location":"info/usage/","title":"Usage","text":"<pre><code>A CLI to aid in upgrading Amazon EKS clusters\n\nUsage: eksup [OPTIONS] &lt;COMMAND&gt;\n\nCommands:\n  analyze  Analyze an Amazon EKS cluster for potential upgrade issues\n  create   Create artifacts using the analysis data\n  help     Print this message or the help of the given subcommand(s)\n\nOptions:\n  -v, --verbose...  Increase logging verbosity\n  -q, --quiet...    Decrease logging verbosity\n  -h, --help        Print help\n  -V, --version     Print version\n</code></pre>"},{"location":"info/usage/#analyze","title":"Analyze","text":"<p>Analyze cluster for any potential issues to remediate prior to upgrade.</p> <pre><code>Analyze an Amazon EKS cluster for potential upgrade issues\n\nUsage: eksup analyze [OPTIONS] --cluster &lt;CLUSTER&gt;\n\nOptions:\n  -c, --cluster &lt;CLUSTER&gt;\n          The name of the cluster to analyze\n\n  -r, --region &lt;REGION&gt;\n          The AWS region where the cluster is provisioned\n\n  -p, --profile &lt;PROFILE&gt;\n          The AWS profile to use to access the cluster\n\n  -v, --verbose...\n          Increase logging verbosity\n\n  -f, --format &lt;FORMAT&gt;\n          Possible values:\n          - json: JSON format used for logging or writing to a *.json file\n          - text: Text format used for writing to stdout\n\n          [default: text]\n\n  -q, --quiet...\n          Decrease logging verbosity\n\n  -o, --output &lt;OUTPUT&gt;\n          Write to file instead of stdout\n\n  -t, --target-version &lt;TARGET_VERSION&gt;\n          Target Kubernetes version for the upgrade (e.g. \"1.34\"). Defaults to current + 1\n\n      --ignore-recommended\n          Exclude recommendations from the output\n\n      --config &lt;CONFIG&gt;\n          Path to an eksup configuration file (default: .eksup.yaml in cwd)\n\n  -h, --help\n          Print help (see a summary with '-h')\n\n  -V, --version\n          Print version\n</code></pre> <p>Show result as plaintext via stdout:</p> <pre><code>eksup analyze --cluster &lt;cluster&gt; --region &lt;region&gt;\n</code></pre> <p>Show result as JSON via stdout:</p> <pre><code>eksup analyze --cluster &lt;cluster&gt; --region &lt;region&gt; --format json\n</code></pre> <p>Save result as plaintext to file:</p> <pre><code>eksup analyze --cluster &lt;cluster&gt; --region &lt;region&gt; --output analysis.txt\n</code></pre> <p>Save result as JSON to S3, ignoring recommendations:</p> <pre><code>eksup analyze --cluster &lt;cluster&gt; --region &lt;region&gt; \\\n  --format json --output s3://&lt;bucket&gt;/&lt;filename&gt; --ignore-recommended\n</code></pre>"},{"location":"info/usage/#create","title":"Create","text":"<p>Create a playbook with analysis findings to guide users through pre-upgrade, upgrade, and post-upgrade process.</p> <p>See <code>examples/test-mixed_v1.24_upgrade.md</code> for an example of a playbook created with <code>eksup</code>.</p> <p>This CLI produces a cluster upgrade playbook that attempts to:</p> <ul> <li>Educate users on the overall process of upgrading an Amazon EKS cluster (order of operations, which parts AWS manages and which parts are the user's responsibility, etc.)</li> <li>Provide one approach as the basis for upgrading a cluster that users can modify/customize to suit their cluster configuration/architecture and business requirements</li> <li>Provide recommendations on what to check for and precautions to consider before upgrading, how to perform the cluster upgrade, and considerations for configuring your cluster and/or applications to minimize risk and disruption during the upgrade process</li> </ul> <pre><code>Create a playbook for upgrading an Amazon EKS cluster\n\nUsage: eksup create playbook [OPTIONS] --cluster &lt;CLUSTER&gt;\n\nOptions:\n  -c, --cluster &lt;CLUSTER&gt;\n          The name of the cluster to analyze\n  -r, --region &lt;REGION&gt;\n          The AWS region where the cluster is provisioned\n  -p, --profile &lt;PROFILE&gt;\n          The AWS profile to use to access the cluster\n  -v, --verbose...\n          Increase logging verbosity\n  -f, --filename &lt;FILENAME&gt;\n          Name of the playbook saved locally\n  -q, --quiet...\n          Decrease logging verbosity\n  -t, --target-version &lt;TARGET_VERSION&gt;\n          Target Kubernetes version for the upgrade (e.g. \"1.34\"). Defaults to current + 1\n      --ignore-recommended\n          Exclude recommendations from the output\n      --config &lt;CONFIG&gt;\n          Path to an eksup configuration file (default: .eksup.yaml in cwd)\n  -h, --help\n          Print help\n  -V, --version\n          Print version\n</code></pre> <p>Create playbook and save locally:</p> <pre><code>eksup create playbook --cluster &lt;cluster&gt; --region &lt;region&gt;\n</code></pre>"},{"location":"info/usage/#configuration","title":"Configuration","text":"<p><code>eksup</code> supports an optional configuration file (<code>.eksup.yaml</code>) for customizing check behavior. By default, <code>eksup</code> looks for <code>.eksup.yaml</code> in the current working directory. You can specify a custom path with the <code>--config</code> flag:</p> <pre><code>eksup analyze --cluster my-cluster --config /path/to/config.yaml\n</code></pre>"},{"location":"info/usage/#configuration-format","title":"Configuration Format","text":"<pre><code>checks:\n  K8S002:\n    min_replicas: 3  # Global minimum replica threshold (default: 2)\n    ignore:          # Resources to skip entirely\n      - name: metrics-server\n        namespace: kube-system\n    overrides:       # Per-resource threshold overrides\n      - name: critical-app\n        namespace: production\n        min_replicas: 5\n  K8S004:\n    ignore:          # Resources to skip PDB check\n      - name: singleton-worker\n        namespace: batch\n</code></pre> <ul> <li><code>K8S002.min_replicas</code>: Global minimum replica threshold (default: 2). Must be &gt;= 1.</li> <li><code>K8S002.ignore</code>: List of resources (by name + namespace) to exclude from the minimum replicas check.</li> <li><code>K8S002.overrides</code>: Per-resource minimum replica threshold. Overrides the global default.</li> <li><code>K8S004.ignore</code>: List of resources (by name + namespace) to exclude from the PodDisruptionBudget check.</li> <li>Ignore takes precedence over overrides when both match the same resource.</li> <li>Unknown fields in the configuration file are rejected with an error.</li> </ul>"}]}
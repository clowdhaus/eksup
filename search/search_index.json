{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EKS Cluster Upgrade Guidance \u00b6 Why Is This Needed \u00b6 Kubernetes releases a new version approximately every 4 months . Each minor version is supported for 12 months after it's first released by the Kubernetes community, and Amazon EKS supports a Kubernetes version for 14 months once made available. In line with the Kubernetes community support for versions, Amazon EKS is committed to supporting at least four versions of Kubernetes at any given time. This means that Amazon EKS users need to be prepared to upgrade their cluster version(s) at least once a year. However, there are a number of factors that can make each upgrade different and unique that users will need to evaluate prior to each upgrade. Factors that can change between each upgrade cycle include: New team members who are inexperienced with the upgrade process, and/or prior team members who have experience in cluster upgrades are no longer on the team Different Kubernetes APIs are marked as deprecated or removed from the next release Kubernetes resources that were previously provided by Kubernetes \"in-tree\" are now provided as external resources (i.e - moving Kubernetes in-tree cloud provider code out to their respective standalone projects such as ALB ingress controller \"in-tree\" to the external ALB load balancer controller) Various changes and deprecations in the components used by Kubernetes (i.e - moving from kube-dns to CoreDNS , moving from Docker engine to containerd for container runtime, dropping support for dockershim , etc.) Changes in your applications, your architecture, or the amount of traffic your clusters are handling. Over time, the number of available IPs for the cluster resources may shrink, stateful workloads may have been added to the cluster, etc., and these factors can influence the upgrade process. What Is eksup \u00b6 eksup is a CLI that helps users prepare for a cluster upgrade - providing users as much relevant information as possible for their upgrade. eksup gives users the ability to analyze their cluster(s) against the next version of Kubernetes, highlighting any findings that may affect the upgrade process. In addition, eksup has the ability to generate a playbook tailored to the cluster analyzed that provides the process for upgrading the cluster including the findings that require remediation. The playbook output allows users to edit the upgrade steps to suit their cluster configuration and business requirements plus capture any specific learnings during the upgrade process. Since most users typically perform upgrades on nonproduction clusters first, any additional steps or call-outs that are discovered during the upgrade process can be captured and used to improve the upgrade process for their production clusters. Users are encouraged to save their playbooks as historical artifacts for future reference to ensure that with each cycle, the team has a better understanding of the upgrade process and more confidence in swiftly working through cluster upgrades before their Kubernetes version support expires. What It Is NOT \u00b6 eksup is not a tool that will perform the cluster upgrade. It is assumed that clusters are generally created using an infrastructure as code approach through tools such as Terraform, eksctl , or CloudFormation. Therefore, users are encouraged to use those tools to perform the upgrade to avoid any resource definition conflicts. It does not perform any modifications on the resources it identifies as needing, or recommending, changes. Again, following the approach of infrastructure as code, users are encouraged to make these changes through their normal change control process at the appropriate time in the upgrade process. In the future, eksup may provide functionality to help in converting a Kubernetes manifest definition from one API version to the next. However, this will occur on the users local filesystem and not against a live cluster. eksup will always operate from the perspective of infrastructure as code; any feature requests that support this tenant are encouraged.","title":"Home"},{"location":"#eks-cluster-upgrade-guidance","text":"","title":"EKS Cluster Upgrade Guidance"},{"location":"#why-is-this-needed","text":"Kubernetes releases a new version approximately every 4 months . Each minor version is supported for 12 months after it's first released by the Kubernetes community, and Amazon EKS supports a Kubernetes version for 14 months once made available. In line with the Kubernetes community support for versions, Amazon EKS is committed to supporting at least four versions of Kubernetes at any given time. This means that Amazon EKS users need to be prepared to upgrade their cluster version(s) at least once a year. However, there are a number of factors that can make each upgrade different and unique that users will need to evaluate prior to each upgrade. Factors that can change between each upgrade cycle include: New team members who are inexperienced with the upgrade process, and/or prior team members who have experience in cluster upgrades are no longer on the team Different Kubernetes APIs are marked as deprecated or removed from the next release Kubernetes resources that were previously provided by Kubernetes \"in-tree\" are now provided as external resources (i.e - moving Kubernetes in-tree cloud provider code out to their respective standalone projects such as ALB ingress controller \"in-tree\" to the external ALB load balancer controller) Various changes and deprecations in the components used by Kubernetes (i.e - moving from kube-dns to CoreDNS , moving from Docker engine to containerd for container runtime, dropping support for dockershim , etc.) Changes in your applications, your architecture, or the amount of traffic your clusters are handling. Over time, the number of available IPs for the cluster resources may shrink, stateful workloads may have been added to the cluster, etc., and these factors can influence the upgrade process.","title":"Why Is This Needed"},{"location":"#what-is-eksup","text":"eksup is a CLI that helps users prepare for a cluster upgrade - providing users as much relevant information as possible for their upgrade. eksup gives users the ability to analyze their cluster(s) against the next version of Kubernetes, highlighting any findings that may affect the upgrade process. In addition, eksup has the ability to generate a playbook tailored to the cluster analyzed that provides the process for upgrading the cluster including the findings that require remediation. The playbook output allows users to edit the upgrade steps to suit their cluster configuration and business requirements plus capture any specific learnings during the upgrade process. Since most users typically perform upgrades on nonproduction clusters first, any additional steps or call-outs that are discovered during the upgrade process can be captured and used to improve the upgrade process for their production clusters. Users are encouraged to save their playbooks as historical artifacts for future reference to ensure that with each cycle, the team has a better understanding of the upgrade process and more confidence in swiftly working through cluster upgrades before their Kubernetes version support expires.","title":"What Is eksup"},{"location":"#what-it-is-not","text":"eksup is not a tool that will perform the cluster upgrade. It is assumed that clusters are generally created using an infrastructure as code approach through tools such as Terraform, eksctl , or CloudFormation. Therefore, users are encouraged to use those tools to perform the upgrade to avoid any resource definition conflicts. It does not perform any modifications on the resources it identifies as needing, or recommending, changes. Again, following the approach of infrastructure as code, users are encouraged to make these changes through their normal change control process at the appropriate time in the upgrade process. In the future, eksup may provide functionality to help in converting a Kubernetes manifest definition from one API version to the next. However, this will occur on the users local filesystem and not against a live cluster. eksup will always operate from the perspective of infrastructure as code; any feature requests that support this tenant are encouraged.","title":"What It Is NOT"},{"location":"scratch/","text":"A check must be able to answer yes to one of the following questions, depending on the type of check: Required: It will adversely affect the cluster and/or the services/applications running on the cluster during an upgrade Recommended: It *has the potential to affect the cluster and/or the services/applications running on the cluster during an upgrade \ud83d\udea7 ToDo \ud83d\udea7 \u00b6 [ ] Add summary at top of results shown to user for stdout and playbook Checks: 31 (Failed: 14, Excluded: 0, Skipped: 0) [x] [ K8S001 ] Version skew between control plane and data plane should adhere to skew policy Amazon EKS \u00b6 [x] [ EKS001 ] There are at least 5 free IPs in control plane subnets [ ] [ AWS001 ] Report on number of free IPs in data plane subnets TBD: should this be reported per MNG/ASG/profile, as a whole (data plane), or both? [x] [ AWS002 ] Report on number of free IPs used by the pods when using custom networking [x] [ EKS002 ] Control plane is free of health issues [x] [ EKS003 ] EKS managed node group(s) are free of health issues [x] [ EKS004 ] EKS addon(s) are free of health issues [x] [ EKS005 ] EKS addon version is within supported range; recommend upgrading if target Kubernetes version default addon version is newer [x] [ EKS006 ] EKS managed node group(s): report if the launch template version is not the latest [x] [ EKS007 ] Self-managed node group(s): report if the launch template version is not the latest [ ] Check AWS service limits and utilization for relevant resources Requires premium support https://docs.aws.amazon.com/awssupport/latest/user/service-limits.html [ ] [ AWS003 ] EC2 instance service limits aws support describe-trusted-advisor-check-result --check-id 0Xc6LMYG8P [ ] EBS volume service limits [ ] [ AWS004 ] GP2 aws support describe-trusted-advisor-check-result --check-id dH7RR0l6J9 [ ] [ AWS005 ] GP3 aws support describe-trusted-advisor-check-result --check-id dH7RR0l6J3 Kubernetes Highly Available \u00b6 Check Deployment ReplicaSet ReplicationController StatefulSet Job CronJob Daemonset [ K8S001 ] - - - - - - - [ K8S002 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S003 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S004 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S005 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S006 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S007 ] \u274c \u274c \u274c \u2705 \u274c \u274c \u274c [ K8S008 ] \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 [ K8S009 ] - - - - - - - [ K8S010 ] - - - - - - - [x] [ K8S002 ] .spec.replicas set >= 3 [x] [ K8S003 ] .spec.minReadySeconds set > 0 - https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes [ ] [ K8S004 ] podDisruptionBudgets set & at least one of minAvailable or maxUnavailable is set [ ] [ K8S005 ] Either .spec.affinity.podAntiAffinity or .spec.topologySpreadConstraints set to avoid multiple pods from being scheduled on the same node. https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ [ ] Prefer topology hints over affinity Note: Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes. https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity [ ] [ K8S006 ] .spec.containers[*].readinessProbe set [ ] .spec.containers[*].livenessProbe , if set, is NOT the same as .spec.containers[*].readinessProbe [ ] .spec.containers[*].startupProbe is set if .spec.containers[*].livenessProbe is set [ ] [ K8S007 ] pod.Spec.TerminationGracePeriodSeconds > 0 - The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0 https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees (StatefulSet) Kubernetes Deprecations \u00b6 Note: the Kubernetes version these apply to will need to be taken into consideration to avoid telling users about checks that do not apply to their version. [ ] [ K8S008 ] Detect docker socket use (1.24+ affected) https://github.com/aws-containers/kubectl-detector-for-docker-socket [ ] [ K8S009 ] Warn on pod security policy use (deprecated 1.21, removed 1.25) https://kubernetes.io/docs/concepts/security/pod-security-policy/ [ ] Advise to switch to pod security admission https://kubernetes.io/docs/concepts/security/pod-security-admission/ [ ] [ K8S010 ] In-tree to CSI migration https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/ ? [ ] The in-tree Amazon EBS storage provisioner is deprecated. If you are upgrading your cluster to version 1.23, then you must first install the Amazon EBS driver before updating your cluster. For more information, see Amazon EBS CSI migration frequently asked questions . If you have pods running on a version 1.22 or earlier cluster, then you must install the Amazon EBS driver before updating your cluster to version 1.23 to avoid service interruption. https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi-migration-faq.html Blog https://aws.amazon.com/blogs/containers/migrating-amazon-eks-clusters-from-gp2-to-gp3-ebs-volumes/ Future Considerations \u00b6 [ ] APIs deprecated and/or removed in the next Kubernetes version For now, pluto or kubent are recommended to check for deprecated APIs Add section on how those tools work, what to watch out for (asking the API Server is not trustworthy, scanning manifests directly is the most accurate) Look into using the apiserve_requested_deprecated_apis metric to detect usage of deprecated APIs https://kubernetes.io/blog/2020/09/03/warnings/ https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1693-warnings https://github.com/kube-rs/kube/issues/492 for implementation [ ] Add image and chart for running eksup on the cluster in a continuous fashion (CronJob) Send results to a central location like S3 for centralized aggregation and reporting across a fleet of clusters [ ] Add support to output results in JSON and CSV formats Multi-cluster scenario - all clusters emitting data back to central location to report on which clusters need components to be upgraded/modified Can utilize an Athena table to aggregate and summarize data [ ] Configuration file to allow users more control over what checks they want to opt in/out of, the values of those checks, etc. [ ] Progress indicator https://github.com/console-rs/indicatif [ ] Ability to convert from one resource API version to another (where possible) migrate or transform Given a manifest, convert the manifest to the next, stable API version. Some resources only need the API version changed, others will require the schema to be modified to match the new API version [ ] Add snippets/information for commonly used provisioning tools to explain how those fit into the guidance terraform-aws-eks / eksctl - how to upgrade a cluster with these tools, what will they do for the user (ensure addon versions are aligned with the Kubernetes version, the ordering of upgrade steps, etc.) [ ] Configure output levels --quiet - suppress all output (default, no flags) - show failed checks on hard requirements --warn - in addition to failed, show warnings (low number of IPs available for nodes/pods, addon version older than current default, etc.) --info - in addition to failed and warnings, show informational notices (number of IPs available for nodes/pods, addon version relative to current default and latest, etc.) Notes \u00b6 Prefer topology hints over affinity for larger clusters Inter-pod affinity and anti-affinity > Note: Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes. Questions \u00b6 What is the guidance for batch workloads? Recommend creating a maintenance window where workloads should avoid being scheduled? JobFailurePolicy coming in in 1.26 https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy What is the recommended way to manage the lifecycle of Fargate pods? After the control plane Kubernetes version has been upgraded, what is the best approach to \"roll\" the pods in order to pull fresh pods/nodes with the new K8s version? Use a mutating webhook to inject nodeSelector: failure-domain.beta.kubernetes.io/zone: <AZ> into pods created to distribute across the AZs. (EKS Fargate does not natively do this today - see https://github.com/aws/containers-roadmap/issues/824) What is the churn calculation for updating node groups? What is the surge calculation - I thought I saw it was 2 * max(min-size, desired-size) somewhere? For EKS MNG, the surge limit is capped at 100 nodes - should this be applied to self-managed node groups as well, and if so, how? This is important for: Do users have enough resources at their disposal before the start their upgrade or do they need to request resource limit increases (EC2s)? How does the number of available IPs affect this process? If a customer knows they only have x available IPs in the data plane (say 100), can we provide a calculation that helps them configure their update settings to avoid errors and exhausting IPs while upgrading? How long will the upgrade take users? How can users influence the amount of churn - why should they, what recommendations or guidance do we have? Do we have different guidance for large clusters? See note on Inter-pod affinity and anti-affinity Kubernetes Upcoming Features \u00b6 Relevant features that are coming in future releases of Kubernetes. A feature is \"relevant\" in in this context if it is something that would be checked and reported on by eksup to aid in upgrades: .spec.updateStrategy.rollingUpdate.maxUnavailable for StatefulSets Kubernetes v1.24 [alpha] Recommend that a value is set on all StatefulSets PodDisruptionCondition for PodDisruptionBudgets Kubernetes v1.26 [beta] See recommendation below for podFailurePolicy .spec.podFailurePolicy for Jobs/CronJobs Kubernetes v1.26 [beta] Recommend to Ignore conditions caused by preemption, API-initiated eviction, or taint-based eviction so that upgrade type evictions do not count against .spec.backoffLimit and the jobs will be re-tried. Note - .spec.restartPolicy will need to be set to Never and PodDisruptionCondition must be set for PodDisruptionBudgets Misc \u00b6 Update strategies should be reviewed for 3rd party addons. For example, once the cluster is upgrade, users should review the other addons/controllers/operators running on the cluster and update if necessary. While not directly tied to the cluster upgrade itself, its important to at least inform that this is a recommended practice for users to follow to avoid downtimes when upgrading those components following a cluster upgrade. .spec.strategy.type != Recreate - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment (Deployment, ReplicaSet) - excludes ReplicationController which recommends blue/green upgrade [ ] .spec.strategy.rollingUpdate.maxUnavailable is set (Recommended) [ ] .spec.strategy.rollingUpdate.maxSurge is set (Recommended) [ ] .spec.updateStrategy.type != OnDelete - https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates (StatefulSet) Data Plane Pre-Upgrade \u00b6 We strongly recommend that you have readiness and liveness probes configured before upgrading the data plane. This ensures that your pods register as ready/healthy at the appropriate time during an upgrade. For stateful workloads \u2139\ufe0f Exploring Upgrade Strategies for Stateful Sets in Kubernetes \u26a0\ufe0f TODO - what guidance for cluster backup before upgrade Velero Portworx 1.24+ only - maximum unavailable pods If your stateful set does not require unique ordering, typically associated with processes that utilize leader election, switching to a parallel strategy for podManagementPolicy will speed up your scale up/down time as well as reduce the time needed to upgrade your cluster. If you are running a critical application on a Karpenter-provisioned node, such as a long running batch job or stateful application, and the node\u2019s TTL has expired, the application will be interrupted when the instance is terminated. By adding a karpenter.sh/do-not-evict annotation to the pod, you are instructing Karpenter to preserve the node until the Pod is terminated or the do-not-evict annotation is removed. See Deprovisioning documentation for further information.","title":"Scratch"},{"location":"scratch/#todo","text":"[ ] Add summary at top of results shown to user for stdout and playbook Checks: 31 (Failed: 14, Excluded: 0, Skipped: 0) [x] [ K8S001 ] Version skew between control plane and data plane should adhere to skew policy","title":"\ud83d\udea7 ToDo \ud83d\udea7"},{"location":"scratch/#amazon-eks","text":"[x] [ EKS001 ] There are at least 5 free IPs in control plane subnets [ ] [ AWS001 ] Report on number of free IPs in data plane subnets TBD: should this be reported per MNG/ASG/profile, as a whole (data plane), or both? [x] [ AWS002 ] Report on number of free IPs used by the pods when using custom networking [x] [ EKS002 ] Control plane is free of health issues [x] [ EKS003 ] EKS managed node group(s) are free of health issues [x] [ EKS004 ] EKS addon(s) are free of health issues [x] [ EKS005 ] EKS addon version is within supported range; recommend upgrading if target Kubernetes version default addon version is newer [x] [ EKS006 ] EKS managed node group(s): report if the launch template version is not the latest [x] [ EKS007 ] Self-managed node group(s): report if the launch template version is not the latest [ ] Check AWS service limits and utilization for relevant resources Requires premium support https://docs.aws.amazon.com/awssupport/latest/user/service-limits.html [ ] [ AWS003 ] EC2 instance service limits aws support describe-trusted-advisor-check-result --check-id 0Xc6LMYG8P [ ] EBS volume service limits [ ] [ AWS004 ] GP2 aws support describe-trusted-advisor-check-result --check-id dH7RR0l6J9 [ ] [ AWS005 ] GP3 aws support describe-trusted-advisor-check-result --check-id dH7RR0l6J3","title":"Amazon EKS"},{"location":"scratch/#kubernetes-highly-available","text":"Check Deployment ReplicaSet ReplicationController StatefulSet Job CronJob Daemonset [ K8S001 ] - - - - - - - [ K8S002 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S003 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S004 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S005 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S006 ] \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c [ K8S007 ] \u274c \u274c \u274c \u2705 \u274c \u274c \u274c [ K8S008 ] \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 [ K8S009 ] - - - - - - - [ K8S010 ] - - - - - - - [x] [ K8S002 ] .spec.replicas set >= 3 [x] [ K8S003 ] .spec.minReadySeconds set > 0 - https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes [ ] [ K8S004 ] podDisruptionBudgets set & at least one of minAvailable or maxUnavailable is set [ ] [ K8S005 ] Either .spec.affinity.podAntiAffinity or .spec.topologySpreadConstraints set to avoid multiple pods from being scheduled on the same node. https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ [ ] Prefer topology hints over affinity Note: Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes. https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity [ ] [ K8S006 ] .spec.containers[*].readinessProbe set [ ] .spec.containers[*].livenessProbe , if set, is NOT the same as .spec.containers[*].readinessProbe [ ] .spec.containers[*].startupProbe is set if .spec.containers[*].livenessProbe is set [ ] [ K8S007 ] pod.Spec.TerminationGracePeriodSeconds > 0 - The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0 https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees (StatefulSet)","title":"Kubernetes Highly Available"},{"location":"scratch/#kubernetes-deprecations","text":"Note: the Kubernetes version these apply to will need to be taken into consideration to avoid telling users about checks that do not apply to their version. [ ] [ K8S008 ] Detect docker socket use (1.24+ affected) https://github.com/aws-containers/kubectl-detector-for-docker-socket [ ] [ K8S009 ] Warn on pod security policy use (deprecated 1.21, removed 1.25) https://kubernetes.io/docs/concepts/security/pod-security-policy/ [ ] Advise to switch to pod security admission https://kubernetes.io/docs/concepts/security/pod-security-admission/ [ ] [ K8S010 ] In-tree to CSI migration https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/ ? [ ] The in-tree Amazon EBS storage provisioner is deprecated. If you are upgrading your cluster to version 1.23, then you must first install the Amazon EBS driver before updating your cluster. For more information, see Amazon EBS CSI migration frequently asked questions . If you have pods running on a version 1.22 or earlier cluster, then you must install the Amazon EBS driver before updating your cluster to version 1.23 to avoid service interruption. https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi-migration-faq.html Blog https://aws.amazon.com/blogs/containers/migrating-amazon-eks-clusters-from-gp2-to-gp3-ebs-volumes/","title":"Kubernetes Deprecations"},{"location":"scratch/#future-considerations","text":"[ ] APIs deprecated and/or removed in the next Kubernetes version For now, pluto or kubent are recommended to check for deprecated APIs Add section on how those tools work, what to watch out for (asking the API Server is not trustworthy, scanning manifests directly is the most accurate) Look into using the apiserve_requested_deprecated_apis metric to detect usage of deprecated APIs https://kubernetes.io/blog/2020/09/03/warnings/ https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1693-warnings https://github.com/kube-rs/kube/issues/492 for implementation [ ] Add image and chart for running eksup on the cluster in a continuous fashion (CronJob) Send results to a central location like S3 for centralized aggregation and reporting across a fleet of clusters [ ] Add support to output results in JSON and CSV formats Multi-cluster scenario - all clusters emitting data back to central location to report on which clusters need components to be upgraded/modified Can utilize an Athena table to aggregate and summarize data [ ] Configuration file to allow users more control over what checks they want to opt in/out of, the values of those checks, etc. [ ] Progress indicator https://github.com/console-rs/indicatif [ ] Ability to convert from one resource API version to another (where possible) migrate or transform Given a manifest, convert the manifest to the next, stable API version. Some resources only need the API version changed, others will require the schema to be modified to match the new API version [ ] Add snippets/information for commonly used provisioning tools to explain how those fit into the guidance terraform-aws-eks / eksctl - how to upgrade a cluster with these tools, what will they do for the user (ensure addon versions are aligned with the Kubernetes version, the ordering of upgrade steps, etc.) [ ] Configure output levels --quiet - suppress all output (default, no flags) - show failed checks on hard requirements --warn - in addition to failed, show warnings (low number of IPs available for nodes/pods, addon version older than current default, etc.) --info - in addition to failed and warnings, show informational notices (number of IPs available for nodes/pods, addon version relative to current default and latest, etc.)","title":"Future Considerations"},{"location":"scratch/#notes","text":"Prefer topology hints over affinity for larger clusters Inter-pod affinity and anti-affinity > Note: Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes.","title":"Notes"},{"location":"scratch/#questions","text":"What is the guidance for batch workloads? Recommend creating a maintenance window where workloads should avoid being scheduled? JobFailurePolicy coming in in 1.26 https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy What is the recommended way to manage the lifecycle of Fargate pods? After the control plane Kubernetes version has been upgraded, what is the best approach to \"roll\" the pods in order to pull fresh pods/nodes with the new K8s version? Use a mutating webhook to inject nodeSelector: failure-domain.beta.kubernetes.io/zone: <AZ> into pods created to distribute across the AZs. (EKS Fargate does not natively do this today - see https://github.com/aws/containers-roadmap/issues/824) What is the churn calculation for updating node groups? What is the surge calculation - I thought I saw it was 2 * max(min-size, desired-size) somewhere? For EKS MNG, the surge limit is capped at 100 nodes - should this be applied to self-managed node groups as well, and if so, how? This is important for: Do users have enough resources at their disposal before the start their upgrade or do they need to request resource limit increases (EC2s)? How does the number of available IPs affect this process? If a customer knows they only have x available IPs in the data plane (say 100), can we provide a calculation that helps them configure their update settings to avoid errors and exhausting IPs while upgrading? How long will the upgrade take users? How can users influence the amount of churn - why should they, what recommendations or guidance do we have? Do we have different guidance for large clusters? See note on Inter-pod affinity and anti-affinity","title":"Questions"},{"location":"scratch/#kubernetes-upcoming-features","text":"Relevant features that are coming in future releases of Kubernetes. A feature is \"relevant\" in in this context if it is something that would be checked and reported on by eksup to aid in upgrades: .spec.updateStrategy.rollingUpdate.maxUnavailable for StatefulSets Kubernetes v1.24 [alpha] Recommend that a value is set on all StatefulSets PodDisruptionCondition for PodDisruptionBudgets Kubernetes v1.26 [beta] See recommendation below for podFailurePolicy .spec.podFailurePolicy for Jobs/CronJobs Kubernetes v1.26 [beta] Recommend to Ignore conditions caused by preemption, API-initiated eviction, or taint-based eviction so that upgrade type evictions do not count against .spec.backoffLimit and the jobs will be re-tried. Note - .spec.restartPolicy will need to be set to Never and PodDisruptionCondition must be set for PodDisruptionBudgets","title":"Kubernetes Upcoming Features"},{"location":"scratch/#misc","text":"Update strategies should be reviewed for 3rd party addons. For example, once the cluster is upgrade, users should review the other addons/controllers/operators running on the cluster and update if necessary. While not directly tied to the cluster upgrade itself, its important to at least inform that this is a recommended practice for users to follow to avoid downtimes when upgrading those components following a cluster upgrade. .spec.strategy.type != Recreate - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment (Deployment, ReplicaSet) - excludes ReplicationController which recommends blue/green upgrade [ ] .spec.strategy.rollingUpdate.maxUnavailable is set (Recommended) [ ] .spec.strategy.rollingUpdate.maxSurge is set (Recommended) [ ] .spec.updateStrategy.type != OnDelete - https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates (StatefulSet)","title":"Misc"},{"location":"scratch/#data-plane-pre-upgrade","text":"We strongly recommend that you have readiness and liveness probes configured before upgrading the data plane. This ensures that your pods register as ready/healthy at the appropriate time during an upgrade. For stateful workloads \u2139\ufe0f Exploring Upgrade Strategies for Stateful Sets in Kubernetes \u26a0\ufe0f TODO - what guidance for cluster backup before upgrade Velero Portworx 1.24+ only - maximum unavailable pods If your stateful set does not require unique ordering, typically associated with processes that utilize leader election, switching to a parallel strategy for podManagementPolicy will speed up your scale up/down time as well as reduce the time needed to upgrade your cluster. If you are running a critical application on a Karpenter-provisioned node, such as a long running batch job or stateful application, and the node\u2019s TTL has expired, the application will be interrupted when the instance is terminated. By adding a karpenter.sh/do-not-evict annotation to the pod, you are instructing Karpenter to preserve the node until the Pod is terminated or the do-not-evict annotation is removed. See Deprovisioning documentation for further information.","title":"Data Plane Pre-Upgrade"},{"location":"design/overview/","text":"Overview \u00b6 Goals \u00b6 There is only one goal: Empower users to routinely upgrade their EKS cluster(s) while avoiding downtime and/or disruption. To aid in that goal, the following are supporting goals or tenants: Intended for use on Amazon EKS clusters; there are no guarantees that this CLI will or will not work on other Kubernetes clusters at this time. The CLI will focus on EKS to avoid the need to support multiple Kubernetes distributions and their associated cloud controller resources and instead focus on the aspects that are specific to EKS and how it manages the Kubernetes experience for users. The CLI should offer native support for: Amazon EKS, Amazon EKS on Outposts (local and extended clusters), and [Not yet supported] EKS-A (EKS Anywhere) Amazon EKS managed node groups, self-managed node groups, and Amazon EKS Fargate profiles EKS addons It is designed to produce the least amount of load on the API Server when discovering and analyzing cluster resources. However, this can have some tradeoffs in terms of accuracy and completeness of the information provided to the user; see goal #3 for more information on this tradeoff. It should provide as much relevant information as possible to the user regarding the state of the cluster prior to upgrading. This includes scoping the information provided to the user to only that which is relevant for upgrading from their current Kubernetes version to the intended target Kubernetes version. A more complete analysis that exhaustively searches ALL Kubernetes resources (i.e. - all pods) will produce more load on the API Server especially as the number of resources in the cluster increases. This is a tradeoff that the CLI will make in order to provide the user with the most relevant information for upgrading: the default mode will use a \"shallow\" search by default, analyzing the higher level constructs such as Deployments, StatefulSets, DaemonSets, etc. This makes the assumption that all pods created in the cluster are created by a higher level construct and therefore only analyzing those resources will be sufficient while reducing the amount of load on the API Server during analysis. [Not yet implemented] The user will have the option to choose to perform a more exhaustive search of all pods in the cluster by using the --deep flag. This will produce more load on the API Server and take longer to complete, but will provide the user with a more complete analysis of the cluster. The exhaustive analysis should be used when the user is not sure if all pods in the cluster are created by a higher level construct. It should support the following use cases: A one-off analysis to create a report of the cluster state prior to upgrading along with steps to take to upgrade the cluster (i.e. - analyze the cluster and generate an upgrade playbook). A one-off analysis to report on the state of the cluster for potential issues and/or recommendations prior to upgrading. This is generally a CLI invocation that prints the analysis to the console (stdout) and exits. [Not yet implemented] Continuous analysis of the cluster state for potential issues and/or recommendations that runs from within the cluster that is being reported on. Results can be sent to stdout where they can be picked up from a log aggregator or sent to a remote location, such as Amazon S3, where they can be analyzed and acted upon. This process supports n-number of clusters across m-number of accounts to better aid in multi-cluster management as well as alerting to ensure enough advance notice is given for users to prepare and schedule the pending upgrade before end of support is reached. Architecture \u00b6 High Level Diagram \u00b6","title":"Overview"},{"location":"design/overview/#overview","text":"","title":"Overview"},{"location":"design/overview/#goals","text":"There is only one goal: Empower users to routinely upgrade their EKS cluster(s) while avoiding downtime and/or disruption. To aid in that goal, the following are supporting goals or tenants: Intended for use on Amazon EKS clusters; there are no guarantees that this CLI will or will not work on other Kubernetes clusters at this time. The CLI will focus on EKS to avoid the need to support multiple Kubernetes distributions and their associated cloud controller resources and instead focus on the aspects that are specific to EKS and how it manages the Kubernetes experience for users. The CLI should offer native support for: Amazon EKS, Amazon EKS on Outposts (local and extended clusters), and [Not yet supported] EKS-A (EKS Anywhere) Amazon EKS managed node groups, self-managed node groups, and Amazon EKS Fargate profiles EKS addons It is designed to produce the least amount of load on the API Server when discovering and analyzing cluster resources. However, this can have some tradeoffs in terms of accuracy and completeness of the information provided to the user; see goal #3 for more information on this tradeoff. It should provide as much relevant information as possible to the user regarding the state of the cluster prior to upgrading. This includes scoping the information provided to the user to only that which is relevant for upgrading from their current Kubernetes version to the intended target Kubernetes version. A more complete analysis that exhaustively searches ALL Kubernetes resources (i.e. - all pods) will produce more load on the API Server especially as the number of resources in the cluster increases. This is a tradeoff that the CLI will make in order to provide the user with the most relevant information for upgrading: the default mode will use a \"shallow\" search by default, analyzing the higher level constructs such as Deployments, StatefulSets, DaemonSets, etc. This makes the assumption that all pods created in the cluster are created by a higher level construct and therefore only analyzing those resources will be sufficient while reducing the amount of load on the API Server during analysis. [Not yet implemented] The user will have the option to choose to perform a more exhaustive search of all pods in the cluster by using the --deep flag. This will produce more load on the API Server and take longer to complete, but will provide the user with a more complete analysis of the cluster. The exhaustive analysis should be used when the user is not sure if all pods in the cluster are created by a higher level construct. It should support the following use cases: A one-off analysis to create a report of the cluster state prior to upgrading along with steps to take to upgrade the cluster (i.e. - analyze the cluster and generate an upgrade playbook). A one-off analysis to report on the state of the cluster for potential issues and/or recommendations prior to upgrading. This is generally a CLI invocation that prints the analysis to the console (stdout) and exits. [Not yet implemented] Continuous analysis of the cluster state for potential issues and/or recommendations that runs from within the cluster that is being reported on. Results can be sent to stdout where they can be picked up from a log aggregator or sent to a remote location, such as Amazon S3, where they can be analyzed and acted upon. This process supports n-number of clusters across m-number of accounts to better aid in multi-cluster management as well as alerting to ensure enough advance notice is given for users to prepare and schedule the pending upgrade before end of support is reached.","title":"Goals"},{"location":"design/overview/#architecture","text":"","title":"Architecture"},{"location":"design/overview/#high-level-diagram","text":"","title":"High Level Diagram"},{"location":"process/","text":"Getting Started \u00b6 Preface \u00b6 Unless otherwise stated, the phrase Amazon EKS cluster or just cluster throughout the documentation typically refers to the control plane. In-place cluster upgrades can only be upgraded to the next incremental minor version. For example, you can upgrade from Kubernetes version 1.20 to 1.21 , but not from 1.20 to 1.22 . Reverting an upgrade, or downgrading the Kubernetes version of a cluster, is not supported. If you upgrade your cluster to a newer Kubernetes version and then want to revert to the previous version, you must create a new,separate cluster and migrate your workloads. If the Amazon EKS cluster primary security group has been deleted, the only course of action to upgrade is to create a new, separate cluster and migrate your workloads. Generally speaking, how well your cluster is configured from a high-availability perspective will determine how well your cluster handles the upgrade process. Ensuring that you have properly configured pod disruption budgets, multiple replicas specified in your deployments and statefulsets, properly configured readiness probes, etc., will help to mitigate potential disruptions or downtime during an upgrade. You can read more about EKS best practices for reliability here . Terminology \u00b6 In-place upgrade : the process of upgrading the associated resource without re-creation. An in-place cluster upgrade means the Amazon EKS cluster is updated while it continues running and serving traffic and it will retain all of its unique attributes such as endpoint, OIDC thumbprints, etc. An in-place nodegroup upgrade consists of upgrading the underlying EC2 instances within the nodegroup without modifying the nodegroup itself. Typically this is performed by providing a new launch template version that contains the new/updated AMI to the nodegroup which will trigger the nodegroup to replace the EC2 instances with new instances to match the updated launch template. In-place upgrades generally use fewer resources than blue/green upgrades, and are typically easier to perform. A downside to an in-place upgrade is the inability to rollback to the prior configuration in some instances, or a longer time to complete the rollback to a prior configuration. Blue/green upgrade : an upgrade strategy where a second resource (green) is created alongside the current resource (blue). Once the new, second (green) resource is ready, workloads and traffic are shifted from the current resource (blue) over to the new, second (green) resource. Once the workloads and traffic have been shifted over, the current resource (blue) is deleted. Blue/green upgrades allow for better risk mitigation during upgrades, especially when there are substantial changes made in the upgrade since the new, second (green) resource can be tested and validate out of band without disrupting workloads or traffic. Only once the new, second (green) resource has been validated should the workload and traffic be shifted over. In addition, if any unforeseen issues do arise once workloads and traffic are on the new, second (green) resource, rollbacks are generally quick and easy to perform since the prior (blue) resource is still available. A downside to a blue/green upgrade is the additional resources required to create the second resource (green) and the additional time required to perform the upgrade. It can take quite a bit more effort to architect and orchestrate blue/green upgrades but the benefits can be well worth it. You can utilize the blue/green upgrade strategy for the entire cluster as well as for nodegroups. See below for an overview on the process of performing a blue/green upgrade on a nodegroup. Overview \u00b6 A high-level overview of the process for upgrading an Amazon EKS cluster in-place consists of: Check for things that will affect the control plane upgrade. This includes checking for any deprecated or removed Kubernetes API objects, ensuring there are at least 5 available IP addresses in the subnets used by the control plane, ensuring the version skew between the control plane and the data plane is within the supported range, etc. Upgrade the control plane to the next incremental minor version of Kubernetes This process will take approximately 15 minutes to complete. Even though Amazon EKS runs a highly available control plane, you might experience minor service interruptions during an update. For example, assume that you attempt to connect to an API server around when it's terminated and replaced by a new API server that's running the new version of Kubernetes. You might experience API call errors or connectivity issues. If this happens, retry your API operations until they succeed. Check for things that will affect the data plane upgrade. This includes checking for any reported health issues on the nodegroups, ensuring there are enough available IPs to perform the upgrade, ensuring the applications running on the data plane are configured for high availability using pod disruption budgets, readiness probes, topology spread constraints, etc. Update the data plane to match the new control plane Kubernetes version Update nodegroups to roll out new AMIs that match the new control plane Kubernetes version; cordon and drain Fargate nodes to have them replaced with new nodes that match the new control plane Kubernetes version Check for any reported health issues on the EKS addons Update the EKS addons Ensure the addons are using a version within the supported range for the new control plane Kubernetes version; ideally, use the default version for the new control plane Kubernetes version Update applications running on the cluster Some application such as cluster-autoscaler , have a versioning scheme that aligns with the Kubernetes version of the cluster they are running on. This means that once the control plane and data plane components have been updated, these applications should be updated to match as well Update any clients that interact with the cluster This includes updating the kubectl client to match the new control plane Kubernetes version This is just a brief overview of the general process for upgrading an Amazon EKS cluster in-place. There are a number of finer details that need to be checked and considered on a per-upgrade basis to ensure the upgrade is successful. This is why eksup was created - to help surface that information to users and provide guidance on the process to upgrade their cluster. Symbol Table \u00b6 Symbol Description \u2139\ufe0f Informational - users are encouraged to familiarize themselves with the information but no action is required to upgrade \u26a0\ufe0f Recommended - users are encouraged to evaluate the recommendation and determine if it is applicable and whether or not to act upon that recommendation. Not remediating the finding does not prevent the upgrade from occurring. \u274c Required - users must remediate the finding prior to upgrading to be able to perform the upgrade and avoid downtime or disruption","title":"Getting Started"},{"location":"process/#getting-started","text":"","title":"Getting Started"},{"location":"process/#preface","text":"Unless otherwise stated, the phrase Amazon EKS cluster or just cluster throughout the documentation typically refers to the control plane. In-place cluster upgrades can only be upgraded to the next incremental minor version. For example, you can upgrade from Kubernetes version 1.20 to 1.21 , but not from 1.20 to 1.22 . Reverting an upgrade, or downgrading the Kubernetes version of a cluster, is not supported. If you upgrade your cluster to a newer Kubernetes version and then want to revert to the previous version, you must create a new,separate cluster and migrate your workloads. If the Amazon EKS cluster primary security group has been deleted, the only course of action to upgrade is to create a new, separate cluster and migrate your workloads. Generally speaking, how well your cluster is configured from a high-availability perspective will determine how well your cluster handles the upgrade process. Ensuring that you have properly configured pod disruption budgets, multiple replicas specified in your deployments and statefulsets, properly configured readiness probes, etc., will help to mitigate potential disruptions or downtime during an upgrade. You can read more about EKS best practices for reliability here .","title":"Preface"},{"location":"process/#terminology","text":"In-place upgrade : the process of upgrading the associated resource without re-creation. An in-place cluster upgrade means the Amazon EKS cluster is updated while it continues running and serving traffic and it will retain all of its unique attributes such as endpoint, OIDC thumbprints, etc. An in-place nodegroup upgrade consists of upgrading the underlying EC2 instances within the nodegroup without modifying the nodegroup itself. Typically this is performed by providing a new launch template version that contains the new/updated AMI to the nodegroup which will trigger the nodegroup to replace the EC2 instances with new instances to match the updated launch template. In-place upgrades generally use fewer resources than blue/green upgrades, and are typically easier to perform. A downside to an in-place upgrade is the inability to rollback to the prior configuration in some instances, or a longer time to complete the rollback to a prior configuration. Blue/green upgrade : an upgrade strategy where a second resource (green) is created alongside the current resource (blue). Once the new, second (green) resource is ready, workloads and traffic are shifted from the current resource (blue) over to the new, second (green) resource. Once the workloads and traffic have been shifted over, the current resource (blue) is deleted. Blue/green upgrades allow for better risk mitigation during upgrades, especially when there are substantial changes made in the upgrade since the new, second (green) resource can be tested and validate out of band without disrupting workloads or traffic. Only once the new, second (green) resource has been validated should the workload and traffic be shifted over. In addition, if any unforeseen issues do arise once workloads and traffic are on the new, second (green) resource, rollbacks are generally quick and easy to perform since the prior (blue) resource is still available. A downside to a blue/green upgrade is the additional resources required to create the second resource (green) and the additional time required to perform the upgrade. It can take quite a bit more effort to architect and orchestrate blue/green upgrades but the benefits can be well worth it. You can utilize the blue/green upgrade strategy for the entire cluster as well as for nodegroups. See below for an overview on the process of performing a blue/green upgrade on a nodegroup.","title":"Terminology"},{"location":"process/#overview","text":"A high-level overview of the process for upgrading an Amazon EKS cluster in-place consists of: Check for things that will affect the control plane upgrade. This includes checking for any deprecated or removed Kubernetes API objects, ensuring there are at least 5 available IP addresses in the subnets used by the control plane, ensuring the version skew between the control plane and the data plane is within the supported range, etc. Upgrade the control plane to the next incremental minor version of Kubernetes This process will take approximately 15 minutes to complete. Even though Amazon EKS runs a highly available control plane, you might experience minor service interruptions during an update. For example, assume that you attempt to connect to an API server around when it's terminated and replaced by a new API server that's running the new version of Kubernetes. You might experience API call errors or connectivity issues. If this happens, retry your API operations until they succeed. Check for things that will affect the data plane upgrade. This includes checking for any reported health issues on the nodegroups, ensuring there are enough available IPs to perform the upgrade, ensuring the applications running on the data plane are configured for high availability using pod disruption budgets, readiness probes, topology spread constraints, etc. Update the data plane to match the new control plane Kubernetes version Update nodegroups to roll out new AMIs that match the new control plane Kubernetes version; cordon and drain Fargate nodes to have them replaced with new nodes that match the new control plane Kubernetes version Check for any reported health issues on the EKS addons Update the EKS addons Ensure the addons are using a version within the supported range for the new control plane Kubernetes version; ideally, use the default version for the new control plane Kubernetes version Update applications running on the cluster Some application such as cluster-autoscaler , have a versioning scheme that aligns with the Kubernetes version of the cluster they are running on. This means that once the control plane and data plane components have been updated, these applications should be updated to match as well Update any clients that interact with the cluster This includes updating the kubectl client to match the new control plane Kubernetes version This is just a brief overview of the general process for upgrading an Amazon EKS cluster in-place. There are a number of finer details that need to be checked and considered on a per-upgrade basis to ensure the upgrade is successful. This is why eksup was created - to help surface that information to users and provide guidance on the process to upgrade their cluster.","title":"Overview"},{"location":"process/#symbol-table","text":"Symbol Description \u2139\ufe0f Informational - users are encouraged to familiarize themselves with the information but no action is required to upgrade \u26a0\ufe0f Recommended - users are encouraged to evaluate the recommendation and determine if it is applicable and whether or not to act upon that recommendation. Not remediating the finding does not prevent the upgrade from occurring. \u274c Required - users must remediate the finding prior to upgrading to be able to perform the upgrade and avoid downtime or disruption","title":"Symbol Table"},{"location":"process/checks/","text":"Checks \u00b6 If a check fails, it is reported as a finding. Each check will have a remediation type - either recommended or required. A recommended remediation is one that is recommended to be performed, but is not required to be performed. \u26a0\ufe0f Recommended: A finding that users are encouraged to evaluate the recommendation and determine if it is applicable and whether or not to act upon that recommendation. Not remediating the finding does not prevent the upgrade from occurring. \u274c Required: A finding that requires remediation prior to upgrading to be able to perform the upgrade and avoid downtime or disruption See the symbol table for further details on the symbols used throughout the documentation. Amazon \u00b6 Checks that are not specific to Amazon EKS or Kubernetes AWS001 \u00b6 \ud83d\udea7 Not yet implemented \u26a0\ufe0f Remediation recommended There is a sufficient quantity of IPs available for the nodes to support the upgrade. If custom networking is enabled, the results represent the number of IPs available in the subnets used by the EC2 instances. Otherwise, the results represent the number of IPs available in the subnets used by both the EC2 instances and the pods. AWS002 \u00b6 \u26a0\ufe0f Remediation recommended There is a sufficient quantity of IPs available for the pods to support the upgrade. This check is used when custom networking is enabled since the IPs used by pods are coming from subnets different from those used by the EC2 instances themselves. AWS003 \u00b6 \ud83d\udea7 Not yet implemented EC2 instance service limits AWS004 \u00b6 \ud83d\udea7 Not yet implemented EBS GP2 volume service limits AWS005 \u00b6 \ud83d\udea7 Not yet implemented EBS GP3 volume service limits Amazon EKS \u00b6 Checks that are specific to Amazon EKS EKS001 \u00b6 \u274c Remediation required There are at least 5 available IPs for the control plane to upgrade; required for cross account ENI creation. EKS002 \u00b6 \u274c Remediation required Control plane does not have any reported health issues. EKS003 \u00b6 \u274c Remediation required EKS managed nodegroup does not have any reported health issues. This does not include self-managed nodegroups or Fargate profiles; those are not currently supported by the AWS API to report health issues. EKS004 \u00b6 \u274c Remediation required EKS addon does not have any reported health issues. EKS005 \u00b6 \u274c Remediation required EKS addon version is within the supported range. The addon must be updated to a version that is supported by the target Kubernetes version prior to upgrading. \u26a0\ufe0f Remediation recommended The target Kubernetes version default addon version is newer than the current addon version. For example, if the default addon version of CoreDNS for Kubernetes v1.24 is v1.8.7-eksbuild.3 and the current addon version is v1.8.4-eksbuild.2 , while the current version is supported on Kubernetes v1.24 , its recommended to update the addon to v1.8.7-eksbuild.3 during the upgrade. EKS006 \u00b6 \u26a0\ufe0f Remediation recommended EKS managed nodegroup are using the latest launch template version and there are no pending updates for the nodegroup. Users are encourage to evaluate if remediation is warranted or not and whether to update to the latest launch template version prior to upgrading. If there are pending updates, this could potentially introduce additional changes to the nodegroup during the upgrade. EKS007 \u00b6 \u26a0\ufe0f Remediation recommended Self-managed nodegroup are using the latest launch template version and there are no pending updates for the nodegroup. Users are encourage to evaluate if remediation is warranted or not and whether to update to the latest launch template version prior to upgrading. If there are pending updates, this could potentially introduce additional changes to the nodegroup during the upgrade. Kubernetes \u00b6 Checks that are specific to Kubernetes, regardless of the underlying platform provider. Table below shows the checks that are applicable, or not, to the respective Kubernetes resource. Check Deployment ReplicaSet ReplicationController StatefulSet Job CronJob Daemonset K8S001 \udb40\udc2d\u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 K8S002 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S003 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S004 \u2705 \u2705 \u274c \u2705 \u274c \u274c \u274c K8S005 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S006 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S007 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S008 \u274c \u274c \u274c \u2705 \u274c \u274c \u274c K8S009 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 K8S010 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 K8S011 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 K8S001 \u00b6 \u274c Remediation required The version skew between the control plane (API Server) and the data plane (kubelet) violates the Kubernetes version skew policy, or will violate the version skew policy after the control plane has been upgraded. The data plane nodes must be upgraded to at least within 1 minor version of the control plane version in order to stay within the version skew policy through the upgrade; it is recommended to upgrade the data plane nodes to the same version as the control plane. \u26a0\ufe0f Remediation recommended There is a version skew between the control plane (API Server) and the data plane (kubelet). While Kubernetes does support a version skew of n-2 between the API Server and kubelet, it is recommended to upgrade the data plane nodes to the same version as the control plane. Kubernetes version skew policy K8S002 \u00b6 \u274c Remediation required There are at least 3 replicas specified for the resource. --- spec : replicas : 3 # >= 3 Multiple replicas, along with the use of PodDisruptionBudget , are required to ensure high availability during the upgrade. EKS Best Practices - Reliability K8S003 \u00b6 \u274c Remediation required minReadySeconds has been set to a value greater than 0 seconds for StatefulSet You can read more about why this is necessary for StatefulSet here \u26a0\ufe0f Remediation recommended minReadySeconds has been set to a value greater than 0 seconds for Deployment , ReplicaSet , ReplicationController K8S004 \u00b6 \ud83d\udea7 Not yet implemented \u274c Remediation required At least one podDisruptionBudget covers the workload, and at least one of minAvailable or maxUnavailable is set The Kubernetes eviction API is the preferred method for draining nodes for replacement during an upgrade. The eviction API respects PodDisruptionBudget and will not evict pods that would violate the PodDisruptionBudget to ensure application availability, when specified. K8S005 \u00b6 \ud83d\udea7 Not yet implemented \u274c Remediation required Either .spec.affinity.podAntiAffinity or .spec.topologySpreadConstraints is set to avoid multiple pods from the same workload from being scheduled on the same node. topologySpreadConstraints are preferred over affinity, especially for larger clusters: Inter-pod affinity and anti-affinity Note: Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes. Types of inter-pod affinity and anti-affinity Pod Topology Spread Constraints K8S006 \u00b6 \ud83d\udea7 Not yet implemented \u274c Remediation required A readinessProbe must be set to ensure traffic is not sent to pods before they are ready following their re-deployment from a node replacement. \u26a0\ufe0f Remediation recommended If a livenessProbe is provided, it should not be the same as readinessProbe , and a startupProbe should also accompany it. K8S007 \u00b6 \ud83d\udea7 Not yet implemented \u274c Remediation required The StatefulSet should not specify a TerminationGracePeriodSeconds of 0 Deployment and Scaling Guarantees The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to force deleting StatefulSet Pods. Force Delete StatefulSet Pods K8S008 \u00b6 \ud83d\udea7 Not yet implemented Pod volumes should not mount the docker.sock file with the removal of the Dockershim starting in Kubernetes v1.24 \u274c Remediation required For clusters on Kubernetes v1.23 \u26a0\ufe0f Remediation recommended For clusters on Kubernetes < v1.22 Dockershim Removal FAQ Detector for Docker Socket (DDS) K8S009 \u00b6 \ud83d\udea7 Not yet implemented The pod security policy resource has been removed started in Kubernetes v1.25 \u274c Remediation required For clusters on Kubernetes v1.24 \u26a0\ufe0f Remediation recommended For clusters on Kubernetes < v1.23 Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller PodSecurityPolicy Deprecation: Past, Present, and Future K8S010 \u00b6 \ud83d\udea7 Not yet implemented The in-tree Amazon EBS storage provisioner is deprecated. If you are upgrading your cluster to version v1.23 , then you must first install the Amazon EBS driver before updating your cluster. For more information, see Amazon EBS CSI migration frequently asked questions . \u274c Remediation required For clusters on Kubernetes v1.22 \u26a0\ufe0f Remediation recommended For clusters on Kubernetes < v1.21 Amazon EBS CSI migration frequently asked questions Kubernetes In-Tree to CSI Volume Migration Status Update","title":"Checks"},{"location":"process/checks/#checks","text":"If a check fails, it is reported as a finding. Each check will have a remediation type - either recommended or required. A recommended remediation is one that is recommended to be performed, but is not required to be performed. \u26a0\ufe0f Recommended: A finding that users are encouraged to evaluate the recommendation and determine if it is applicable and whether or not to act upon that recommendation. Not remediating the finding does not prevent the upgrade from occurring. \u274c Required: A finding that requires remediation prior to upgrading to be able to perform the upgrade and avoid downtime or disruption See the symbol table for further details on the symbols used throughout the documentation.","title":"Checks"},{"location":"process/checks/#amazon","text":"Checks that are not specific to Amazon EKS or Kubernetes","title":"Amazon"},{"location":"process/checks/#aws001","text":"\ud83d\udea7 Not yet implemented \u26a0\ufe0f Remediation recommended There is a sufficient quantity of IPs available for the nodes to support the upgrade. If custom networking is enabled, the results represent the number of IPs available in the subnets used by the EC2 instances. Otherwise, the results represent the number of IPs available in the subnets used by both the EC2 instances and the pods.","title":"AWS001"},{"location":"process/checks/#aws002","text":"\u26a0\ufe0f Remediation recommended There is a sufficient quantity of IPs available for the pods to support the upgrade. This check is used when custom networking is enabled since the IPs used by pods are coming from subnets different from those used by the EC2 instances themselves.","title":"AWS002"},{"location":"process/checks/#aws003","text":"\ud83d\udea7 Not yet implemented EC2 instance service limits","title":"AWS003"},{"location":"process/checks/#aws004","text":"\ud83d\udea7 Not yet implemented EBS GP2 volume service limits","title":"AWS004"},{"location":"process/checks/#aws005","text":"\ud83d\udea7 Not yet implemented EBS GP3 volume service limits","title":"AWS005"},{"location":"process/checks/#amazon-eks","text":"Checks that are specific to Amazon EKS","title":"Amazon EKS"},{"location":"process/checks/#eks001","text":"\u274c Remediation required There are at least 5 available IPs for the control plane to upgrade; required for cross account ENI creation.","title":"EKS001"},{"location":"process/checks/#eks002","text":"\u274c Remediation required Control plane does not have any reported health issues.","title":"EKS002"},{"location":"process/checks/#eks003","text":"\u274c Remediation required EKS managed nodegroup does not have any reported health issues. This does not include self-managed nodegroups or Fargate profiles; those are not currently supported by the AWS API to report health issues.","title":"EKS003"},{"location":"process/checks/#eks004","text":"\u274c Remediation required EKS addon does not have any reported health issues.","title":"EKS004"},{"location":"process/checks/#eks005","text":"\u274c Remediation required EKS addon version is within the supported range. The addon must be updated to a version that is supported by the target Kubernetes version prior to upgrading. \u26a0\ufe0f Remediation recommended The target Kubernetes version default addon version is newer than the current addon version. For example, if the default addon version of CoreDNS for Kubernetes v1.24 is v1.8.7-eksbuild.3 and the current addon version is v1.8.4-eksbuild.2 , while the current version is supported on Kubernetes v1.24 , its recommended to update the addon to v1.8.7-eksbuild.3 during the upgrade.","title":"EKS005"},{"location":"process/checks/#eks006","text":"\u26a0\ufe0f Remediation recommended EKS managed nodegroup are using the latest launch template version and there are no pending updates for the nodegroup. Users are encourage to evaluate if remediation is warranted or not and whether to update to the latest launch template version prior to upgrading. If there are pending updates, this could potentially introduce additional changes to the nodegroup during the upgrade.","title":"EKS006"},{"location":"process/checks/#eks007","text":"\u26a0\ufe0f Remediation recommended Self-managed nodegroup are using the latest launch template version and there are no pending updates for the nodegroup. Users are encourage to evaluate if remediation is warranted or not and whether to update to the latest launch template version prior to upgrading. If there are pending updates, this could potentially introduce additional changes to the nodegroup during the upgrade.","title":"EKS007"},{"location":"process/checks/#kubernetes","text":"Checks that are specific to Kubernetes, regardless of the underlying platform provider. Table below shows the checks that are applicable, or not, to the respective Kubernetes resource. Check Deployment ReplicaSet ReplicationController StatefulSet Job CronJob Daemonset K8S001 \udb40\udc2d\u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 K8S002 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S003 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S004 \u2705 \u2705 \u274c \u2705 \u274c \u274c \u274c K8S005 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S006 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S007 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c K8S008 \u274c \u274c \u274c \u2705 \u274c \u274c \u274c K8S009 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 K8S010 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 K8S011 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796 \u2796","title":"Kubernetes"},{"location":"process/checks/#k8s001","text":"\u274c Remediation required The version skew between the control plane (API Server) and the data plane (kubelet) violates the Kubernetes version skew policy, or will violate the version skew policy after the control plane has been upgraded. The data plane nodes must be upgraded to at least within 1 minor version of the control plane version in order to stay within the version skew policy through the upgrade; it is recommended to upgrade the data plane nodes to the same version as the control plane. \u26a0\ufe0f Remediation recommended There is a version skew between the control plane (API Server) and the data plane (kubelet). While Kubernetes does support a version skew of n-2 between the API Server and kubelet, it is recommended to upgrade the data plane nodes to the same version as the control plane. Kubernetes version skew policy","title":"K8S001"},{"location":"process/checks/#k8s002","text":"\u274c Remediation required There are at least 3 replicas specified for the resource. --- spec : replicas : 3 # >= 3 Multiple replicas, along with the use of PodDisruptionBudget , are required to ensure high availability during the upgrade. EKS Best Practices - Reliability","title":"K8S002"},{"location":"process/checks/#k8s003","text":"\u274c Remediation required minReadySeconds has been set to a value greater than 0 seconds for StatefulSet You can read more about why this is necessary for StatefulSet here \u26a0\ufe0f Remediation recommended minReadySeconds has been set to a value greater than 0 seconds for Deployment , ReplicaSet , ReplicationController","title":"K8S003"},{"location":"process/checks/#k8s004","text":"\ud83d\udea7 Not yet implemented \u274c Remediation required At least one podDisruptionBudget covers the workload, and at least one of minAvailable or maxUnavailable is set The Kubernetes eviction API is the preferred method for draining nodes for replacement during an upgrade. The eviction API respects PodDisruptionBudget and will not evict pods that would violate the PodDisruptionBudget to ensure application availability, when specified.","title":"K8S004"},{"location":"process/checks/#k8s005","text":"\ud83d\udea7 Not yet implemented \u274c Remediation required Either .spec.affinity.podAntiAffinity or .spec.topologySpreadConstraints is set to avoid multiple pods from the same workload from being scheduled on the same node. topologySpreadConstraints are preferred over affinity, especially for larger clusters: Inter-pod affinity and anti-affinity Note: Inter-pod affinity and anti-affinity require substantial amount of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes. Types of inter-pod affinity and anti-affinity Pod Topology Spread Constraints","title":"K8S005"},{"location":"process/checks/#k8s006","text":"\ud83d\udea7 Not yet implemented \u274c Remediation required A readinessProbe must be set to ensure traffic is not sent to pods before they are ready following their re-deployment from a node replacement. \u26a0\ufe0f Remediation recommended If a livenessProbe is provided, it should not be the same as readinessProbe , and a startupProbe should also accompany it.","title":"K8S006"},{"location":"process/checks/#k8s007","text":"\ud83d\udea7 Not yet implemented \u274c Remediation required The StatefulSet should not specify a TerminationGracePeriodSeconds of 0 Deployment and Scaling Guarantees The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to force deleting StatefulSet Pods. Force Delete StatefulSet Pods","title":"K8S007"},{"location":"process/checks/#k8s008","text":"\ud83d\udea7 Not yet implemented Pod volumes should not mount the docker.sock file with the removal of the Dockershim starting in Kubernetes v1.24 \u274c Remediation required For clusters on Kubernetes v1.23 \u26a0\ufe0f Remediation recommended For clusters on Kubernetes < v1.22 Dockershim Removal FAQ Detector for Docker Socket (DDS)","title":"K8S008"},{"location":"process/checks/#k8s009","text":"\ud83d\udea7 Not yet implemented The pod security policy resource has been removed started in Kubernetes v1.25 \u274c Remediation required For clusters on Kubernetes v1.24 \u26a0\ufe0f Remediation recommended For clusters on Kubernetes < v1.23 Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller PodSecurityPolicy Deprecation: Past, Present, and Future","title":"K8S009"},{"location":"process/checks/#k8s010","text":"\ud83d\udea7 Not yet implemented The in-tree Amazon EBS storage provisioner is deprecated. If you are upgrading your cluster to version v1.23 , then you must first install the Amazon EBS driver before updating your cluster. For more information, see Amazon EBS CSI migration frequently asked questions . \u274c Remediation required For clusters on Kubernetes v1.22 \u26a0\ufe0f Remediation recommended For clusters on Kubernetes < v1.21 Amazon EBS CSI migration frequently asked questions Kubernetes In-Tree to CSI Volume Migration Status Update","title":"K8S010"},{"location":"process/commands/","text":"Commands \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 A CLI to aid in upgrading Amazon EKS clusters Usage: eksup <COMMAND> Commands: analyze Analyze an Amazon EKS cluster for potential upgrade issues create Create artifacts using the analysis data help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version Analyze \u00b6 Analyze cluster for any potential issues to remediate prior to upgrade. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Analyze an Amazon EKS cluster for potential upgrade issues Usage: eksup analyze [OPTIONS] --cluster <CLUSTER> Options: -c, --cluster <CLUSTER> The name of the cluster to analyze -r, --region <REGION> The AWS region where the cluster is provisioned -f, --format <FORMAT> [default: text] Possible values: - json: JSON format used for logging or writing to a *.json file - text: Text format used for writing to stdout -o, --output <OUTPUT> Write to file instead of stdout --ignore-recommended Exclude recommendations from the output -h, --help Print help (see a summary with '-h') -V, --version Print version Show result as plaintext via stdout: 1 eksup analyze --cluster <cluster> --region <region> Show result as JSON via stdout: 1 eksup analyze --cluster <cluster> --region <region> --format json Save result as plaintext to file: 1 eksup analyze --cluster <cluster> --region <region> --output analysis.txt Save result as JSON to S3, ignoring recommendations: 1 2 eksup analyze --cluster <cluster> --region <region> \\ --format json --output s3://<bucket>/<filename> --ignore-recommended Create \u00b6 Create a playbook with analysis findings to guide users through pre-upgrade, upgrade, and post-upgrade process. This CLI produces a cluster upgrade playbook that attempts to: Educate users on the overall process of upgrading an Amazon EKS cluster (order of operations, which parts AWS manages and which parts are the user's responsibility, etc.) Provide one approach as the basis for upgrading a cluster that users can modify/customize to suit their cluster configuration/architecture and business requirements Provide recommendations on what to check for and precautions to consider before upgrading, how to perform the cluster upgrade, and considerations for configuring your cluster and/or applications to minimize risk and disruption during the upgrade process 1 2 3 4 5 6 7 8 9 10 11 Create a playbook for upgrading an Amazon EKS cluster Usage: eksup create playbook [OPTIONS] --cluster <CLUSTER> Options: -c, --cluster <CLUSTER> The name of the cluster to analyze -r, --region <REGION> The AWS region where the cluster is provisioned -f, --filename <FILENAME> Name of the playbook saved locally --ignore-recommended Exclude recommendations from the output -h, --help Print help -V, --version Print version Create playbook and save locally: 1 eksup create playbook --cluster <cluster> --region <region> Create playbook and save locally, ignoring recommendations: 1 eksup create playbook --cluster <cluster> --region <region> --ignore-recommended See examples for examples of the playbook output.","title":"Commands"},{"location":"process/commands/#commands","text":"1 2 3 4 5 6 7 8 9 10 11 12 A CLI to aid in upgrading Amazon EKS clusters Usage: eksup <COMMAND> Commands: analyze Analyze an Amazon EKS cluster for potential upgrade issues create Create artifacts using the analysis data help Print this message or the help of the given subcommand(s) Options: -h, --help Print help -V, --version Print version","title":"Commands"},{"location":"process/commands/#analyze","text":"Analyze cluster for any potential issues to remediate prior to upgrade. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Analyze an Amazon EKS cluster for potential upgrade issues Usage: eksup analyze [OPTIONS] --cluster <CLUSTER> Options: -c, --cluster <CLUSTER> The name of the cluster to analyze -r, --region <REGION> The AWS region where the cluster is provisioned -f, --format <FORMAT> [default: text] Possible values: - json: JSON format used for logging or writing to a *.json file - text: Text format used for writing to stdout -o, --output <OUTPUT> Write to file instead of stdout --ignore-recommended Exclude recommendations from the output -h, --help Print help (see a summary with '-h') -V, --version Print version Show result as plaintext via stdout: 1 eksup analyze --cluster <cluster> --region <region> Show result as JSON via stdout: 1 eksup analyze --cluster <cluster> --region <region> --format json Save result as plaintext to file: 1 eksup analyze --cluster <cluster> --region <region> --output analysis.txt Save result as JSON to S3, ignoring recommendations: 1 2 eksup analyze --cluster <cluster> --region <region> \\ --format json --output s3://<bucket>/<filename> --ignore-recommended","title":"Analyze"},{"location":"process/commands/#create","text":"Create a playbook with analysis findings to guide users through pre-upgrade, upgrade, and post-upgrade process. This CLI produces a cluster upgrade playbook that attempts to: Educate users on the overall process of upgrading an Amazon EKS cluster (order of operations, which parts AWS manages and which parts are the user's responsibility, etc.) Provide one approach as the basis for upgrading a cluster that users can modify/customize to suit their cluster configuration/architecture and business requirements Provide recommendations on what to check for and precautions to consider before upgrading, how to perform the cluster upgrade, and considerations for configuring your cluster and/or applications to minimize risk and disruption during the upgrade process 1 2 3 4 5 6 7 8 9 10 11 Create a playbook for upgrading an Amazon EKS cluster Usage: eksup create playbook [OPTIONS] --cluster <CLUSTER> Options: -c, --cluster <CLUSTER> The name of the cluster to analyze -r, --region <REGION> The AWS region where the cluster is provisioned -f, --filename <FILENAME> Name of the playbook saved locally --ignore-recommended Exclude recommendations from the output -h, --help Print help -V, --version Print version Create playbook and save locally: 1 eksup create playbook --cluster <cluster> --region <region> Create playbook and save locally, ignoring recommendations: 1 eksup create playbook --cluster <cluster> --region <region> --ignore-recommended See examples for examples of the playbook output.","title":"Create"}]}